# abstract_battle

## 简介

石头剪刀布（Rock-Scissors-Paper，以下简称RSP）是一种简单的博弈游戏，石头克制剪刀，剪刀克制布，纳什均衡下的混合策略是石头，剪刀，布的出手概率均为1/3。接下来，我们适当扩展一下石头剪刀布的规则，定义以下规则：

对战双方各拥有5点hp，1点初始攻击力，并且拥有以下三种动作：

- 攻击：若攻击成功，会对对方hp造成攻击力的伤害
- 防守：防御会使对方攻击无效，并且恢复对应的hp；连续防御成功率会减半
- 充能：攻击力上升1

这样就构成了一个简单的类RSP博弈，并且这种博弈是很多指令回合制RPG的原型，接下来，我们用强化学习的方法来求解一下该游戏的纳什均衡。

## 神经虚拟自博弈（NFSP）

网上貌似关于NFSP的资料很少，NFSP是虚拟自博弈（Fictious Self Play，简称FSP）的神经网络版本，FSP是虚拟博弈（Fictious Play，简称FP）的改进版本，这里我将简单介绍一下这一类方法的思想。

在现实中，两位玩家会经过不断地相互博弈后，达到纳什均衡。FP沿用了这个思路，我们以RSP为例，讲解一下FP是怎样达到纳什均衡的。

我们假设有两个玩家A，B，他们会记得对方的策略，并且据此调整自己的策略。A的最初策略是一直出剪刀，B最初的策略是一直出石头。

在两位交战了10回合之后，A总结出B出石头的概率是100%，B总结出A出剪刀的概率是100%，于是A改进了自己的策略，决定一直出布；B知道自己的策略暂时占优所以不改动策略。又交战了10回合，B总结出A出剪刀的概率为50%，出布的概率是50%，于是决定一直出剪刀，因为一直出剪刀自己不会输；在不断地交战，不断地改进策略后，两人最后的策略会停留在出石头，剪刀，布的出手概率均为1/3。

我们会发现FP适用的场景为这种标准的单回合博弈（Normal-form game），而FSP将FP的思想拓展到序列决策的博弈中，比如我们上面定义的这种游戏中。FSP的核心思路延续自FP：

1. 根据对手的策略寻找最优策略

2. 综合自己的最优策略，得到平均策略

使用平均策略的原因，是因为只寻找最优策略会很容易过拟合，造成自己的策略失去随机性。

NFSP使用了深度强化学习替换步骤1，深度学习替换步骤2，从而得到的深度学习模型即是我们想要的最终模型。

## 实验  

因此，我们使用一个Dual-DQN作为强化学习模型（TODO：DQN稳定性不好，之后替换成PPO），胜利的reward为1，失败为-1，平局为0.5，一个两层的全连接网络作为平均策略模型，经过100w回合的对战之后，训练loss如下：

![loss](fig/loss.png)

无论是A还是B，loss长得都是差不多的，RL loss先上升后下降是因为在RL中策略使用的是$\epsilon-$greedy的方式，$\epsilon$值随回合数不断下降，因此后面RL的loss才不断下降。

![loss](fig/ratio.png)

两个模型对战一直处于五五开的局面，因此也没有出现一个模型碾压另一个模型的过拟合的情况。

![loss](fig/val_ratio.png)

作为验证，每1000回合，训练出来的平均策略就会和一个完全随机出招的敌人进行对决，我们可以看到胜率在50%左右波动。

接下来，我们查看平均策略的具体内容，我们会用一个状态向量来描述当前回合双方的状态，比如(5,1,1,5,1,1)，表示：

| 5       | 1         | 1.0           | 5       | 1         | 1.0           |
| ------- | --------- | ------------- | ------- | --------- | ------------- |
| B的血量 | B的攻击力 | B的防御成功率 | A的血量 | A的攻击力 | A的防御成功率 |

A，B两个模型形成的平均策略如下：

第一回合：(5,1,1,5,1,1)

|       | 攻击   | 防守   | 充能   |
| ----- | ------ | ------ | ------ |
| 模型A | 0.3975 | 0.0574 | 0.5451 |
| 模型B | 0.3438 | 0.0503 | 0.6059 |

可以看到，两个模型在第一回合基本不会选择防守，大概率选择充能，小概率选择攻击。这是合理的，因为在第一回合防守的收益是偏小的，假如防守成功，hp也不会高于5，比残血时防守要亏。

假设双方均选择充能：

第二回合：(5,2,1,5,2,1)

|       | 攻击   | 防守   | 充能   |
| ----- | ------ | ------ | ------ |
| 模型A | 0.0656 | 0.0399 | 0.8945 |
| 模型B | 0.8884 | 0.0497 | 0.0618 |

两者的行为发生了变化，A倾向于继续充能，B倾向于攻击。

我们假设A选择了充能，B选择了攻击

第三回合：(5,1,1,3,3,1)

|       | 攻击   | 防守   | 充能   |
| ----- | ------ | ------ | ------ |
| 模型A | 0.9200 | 0.0379 | 0.0422 |
| 模型B | 0.9069 | 0.0473 | 0.0458 |

我们继续假设双方继续攻击。

第四回合：(2,1,1,2,1,1)

|       | 攻击   | 防守   | 充能   |
| ----- | ------ | ------ | ------ |
| 模型A | 0.9076 | 0.0475 | 0.0449 |
| 模型B | 0.9093 | 0.0522 | 0.0385 |

我们继续假设双方继续攻击。

第五回合：(1,1,1,1,1,1)

|       | 攻击   | 防守   | 充能   |
| ----- | ------ | ------ | ------ |
| 模型A | 0.8914 | 0.0549 | 0.0537 |
| 模型B | 0.9083 | 0.0512 | 0.0405 |

至此，双方hp归零，平局结束。

我们可以看到，双方形成的平均策略即是全力进攻，毫不防守。但这应该远远还没达到纳什均衡的程度，因为我们可以很简单的发现，假如有一个猥琐的敌人，采取打一下，防守一下的策略，将完封A和B，经过测试，如果面对这么一个猥琐的敌人，A和B的胜率将会降到20%左右。因此，模型还有继续训练的空间，但是之前在继续训练更多回合的时候发现，训练并不稳定，容易发生过拟合，因此下一步的计划是更改RL部分的方法。
